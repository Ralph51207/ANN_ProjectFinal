{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6879914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE  # if using oversampling\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "890cc327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc7156ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load data\n",
    "df = pd.read_csv('Synthetic_Financial_datasets_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87a38787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATASET OVERVIEW\n",
      "==================================================\n",
      "\n",
      "Dataset shape: (6362620, 11)\n",
      "Number of rows: 6362620\n",
      "Number of columns: 11\n",
      "\n",
      "==================================================\n",
      "COLUMN NAMES\n",
      "==================================================\n",
      "['step', 'type', 'amount', 'nameOrig', 'oldbalanceOrg', 'newbalanceOrig', 'nameDest', 'oldbalanceDest', 'newbalanceDest', 'isFraud', 'isFlaggedFraud']\n",
      "\n",
      "==================================================\n",
      "DATA TYPES\n",
      "==================================================\n",
      "step                int64\n",
      "type               object\n",
      "amount            float64\n",
      "nameOrig           object\n",
      "oldbalanceOrg     float64\n",
      "newbalanceOrig    float64\n",
      "nameDest           object\n",
      "oldbalanceDest    float64\n",
      "newbalanceDest    float64\n",
      "isFraud             int64\n",
      "isFlaggedFraud      int64\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "FIRST 5 ROWS\n",
      "==================================================\n",
      "   step      type    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
      "0     1   PAYMENT   9839.64  C1231006815       170136.0       160296.36   \n",
      "1     1   PAYMENT   1864.28  C1666544295        21249.0        19384.72   \n",
      "2     1  TRANSFER    181.00  C1305486145          181.0            0.00   \n",
      "3     1  CASH_OUT    181.00   C840083671          181.0            0.00   \n",
      "4     1   PAYMENT  11668.14  C2048537720        41554.0        29885.86   \n",
      "\n",
      "      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \n",
      "0  M1979787155             0.0             0.0        0               0  \n",
      "1  M2044282225             0.0             0.0        0               0  \n",
      "2   C553264065             0.0             0.0        1               0  \n",
      "3    C38997010         21182.0             0.0        1               0  \n",
      "4  M1230701703             0.0             0.0        0               0  \n",
      "\n",
      "==================================================\n",
      "BASIC STATISTICS\n",
      "==================================================\n",
      "               step        amount  oldbalanceOrg  newbalanceOrig  \\\n",
      "count  6.362620e+06  6.362620e+06   6.362620e+06    6.362620e+06   \n",
      "mean   2.433972e+02  1.798619e+05   8.338831e+05    8.551137e+05   \n",
      "std    1.423320e+02  6.038582e+05   2.888243e+06    2.924049e+06   \n",
      "min    1.000000e+00  0.000000e+00   0.000000e+00    0.000000e+00   \n",
      "25%    1.560000e+02  1.338957e+04   0.000000e+00    0.000000e+00   \n",
      "50%    2.390000e+02  7.487194e+04   1.420800e+04    0.000000e+00   \n",
      "75%    3.350000e+02  2.087215e+05   1.073152e+05    1.442584e+05   \n",
      "max    7.430000e+02  9.244552e+07   5.958504e+07    4.958504e+07   \n",
      "\n",
      "       oldbalanceDest  newbalanceDest       isFraud  isFlaggedFraud  \n",
      "count    6.362620e+06    6.362620e+06  6.362620e+06    6.362620e+06  \n",
      "mean     1.100702e+06    1.224996e+06  1.290820e-03    2.514687e-06  \n",
      "std      3.399180e+06    3.674129e+06  3.590480e-02    1.585775e-03  \n",
      "min      0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00  \n",
      "25%      0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00  \n",
      "50%      1.327057e+05    2.146614e+05  0.000000e+00    0.000000e+00  \n",
      "75%      9.430367e+05    1.111909e+06  0.000000e+00    0.000000e+00  \n",
      "max      3.560159e+08    3.561793e+08  1.000000e+00    1.000000e+00  \n",
      "\n",
      "==================================================\n",
      "MISSING VALUES\n",
      "==================================================\n",
      "               step        amount  oldbalanceOrg  newbalanceOrig  \\\n",
      "count  6.362620e+06  6.362620e+06   6.362620e+06    6.362620e+06   \n",
      "mean   2.433972e+02  1.798619e+05   8.338831e+05    8.551137e+05   \n",
      "std    1.423320e+02  6.038582e+05   2.888243e+06    2.924049e+06   \n",
      "min    1.000000e+00  0.000000e+00   0.000000e+00    0.000000e+00   \n",
      "25%    1.560000e+02  1.338957e+04   0.000000e+00    0.000000e+00   \n",
      "50%    2.390000e+02  7.487194e+04   1.420800e+04    0.000000e+00   \n",
      "75%    3.350000e+02  2.087215e+05   1.073152e+05    1.442584e+05   \n",
      "max    7.430000e+02  9.244552e+07   5.958504e+07    4.958504e+07   \n",
      "\n",
      "       oldbalanceDest  newbalanceDest       isFraud  isFlaggedFraud  \n",
      "count    6.362620e+06    6.362620e+06  6.362620e+06    6.362620e+06  \n",
      "mean     1.100702e+06    1.224996e+06  1.290820e-03    2.514687e-06  \n",
      "std      3.399180e+06    3.674129e+06  3.590480e-02    1.585775e-03  \n",
      "min      0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00  \n",
      "25%      0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00  \n",
      "50%      1.327057e+05    2.146614e+05  0.000000e+00    0.000000e+00  \n",
      "75%      9.430367e+05    1.111909e+06  0.000000e+00    0.000000e+00  \n",
      "max      3.560159e+08    3.561793e+08  1.000000e+00    1.000000e+00  \n",
      "\n",
      "==================================================\n",
      "MISSING VALUES\n",
      "==================================================\n",
      "               step        amount  oldbalanceOrg  newbalanceOrig  \\\n",
      "count  6.362620e+06  6.362620e+06   6.362620e+06    6.362620e+06   \n",
      "mean   2.433972e+02  1.798619e+05   8.338831e+05    8.551137e+05   \n",
      "std    1.423320e+02  6.038582e+05   2.888243e+06    2.924049e+06   \n",
      "min    1.000000e+00  0.000000e+00   0.000000e+00    0.000000e+00   \n",
      "25%    1.560000e+02  1.338957e+04   0.000000e+00    0.000000e+00   \n",
      "50%    2.390000e+02  7.487194e+04   1.420800e+04    0.000000e+00   \n",
      "75%    3.350000e+02  2.087215e+05   1.073152e+05    1.442584e+05   \n",
      "max    7.430000e+02  9.244552e+07   5.958504e+07    4.958504e+07   \n",
      "\n",
      "       oldbalanceDest  newbalanceDest       isFraud  isFlaggedFraud  \n",
      "count    6.362620e+06    6.362620e+06  6.362620e+06    6.362620e+06  \n",
      "mean     1.100702e+06    1.224996e+06  1.290820e-03    2.514687e-06  \n",
      "std      3.399180e+06    3.674129e+06  3.590480e-02    1.585775e-03  \n",
      "min      0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00  \n",
      "25%      0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00  \n",
      "50%      1.327057e+05    2.146614e+05  0.000000e+00    0.000000e+00  \n",
      "75%      9.430367e+05    1.111909e+06  0.000000e+00    0.000000e+00  \n",
      "max      3.560159e+08    3.561793e+08  1.000000e+00    1.000000e+00  \n",
      "\n",
      "==================================================\n",
      "MISSING VALUES\n",
      "==================================================\n",
      "step              0\n",
      "type              0\n",
      "amount            0\n",
      "nameOrig          0\n",
      "oldbalanceOrg     0\n",
      "newbalanceOrig    0\n",
      "nameDest          0\n",
      "oldbalanceDest    0\n",
      "newbalanceDest    0\n",
      "isFraud           0\n",
      "isFlaggedFraud    0\n",
      "dtype: int64\n",
      "\n",
      "==================================================\n",
      "CATEGORICAL COLUMNS (object type)\n",
      "==================================================\n",
      "['type', 'nameOrig', 'nameDest']\n",
      "\n",
      "type - Unique values:\n",
      "step              0\n",
      "type              0\n",
      "amount            0\n",
      "nameOrig          0\n",
      "oldbalanceOrg     0\n",
      "newbalanceOrig    0\n",
      "nameDest          0\n",
      "oldbalanceDest    0\n",
      "newbalanceDest    0\n",
      "isFraud           0\n",
      "isFlaggedFraud    0\n",
      "dtype: int64\n",
      "\n",
      "==================================================\n",
      "CATEGORICAL COLUMNS (object type)\n",
      "==================================================\n",
      "['type', 'nameOrig', 'nameDest']\n",
      "\n",
      "type - Unique values:\n",
      "step              0\n",
      "type              0\n",
      "amount            0\n",
      "nameOrig          0\n",
      "oldbalanceOrg     0\n",
      "newbalanceOrig    0\n",
      "nameDest          0\n",
      "oldbalanceDest    0\n",
      "newbalanceDest    0\n",
      "isFraud           0\n",
      "isFlaggedFraud    0\n",
      "dtype: int64\n",
      "\n",
      "==================================================\n",
      "CATEGORICAL COLUMNS (object type)\n",
      "==================================================\n",
      "['type', 'nameOrig', 'nameDest']\n",
      "\n",
      "type - Unique values:\n",
      "type\n",
      "CASH_OUT    2237500\n",
      "PAYMENT     2151495\n",
      "CASH_IN     1399284\n",
      "TRANSFER     532909\n",
      "DEBIT         41432\n",
      "Name: count, dtype: int64\n",
      "\n",
      "nameOrig - Unique values:\n",
      "type\n",
      "CASH_OUT    2237500\n",
      "PAYMENT     2151495\n",
      "CASH_IN     1399284\n",
      "TRANSFER     532909\n",
      "DEBIT         41432\n",
      "Name: count, dtype: int64\n",
      "\n",
      "nameOrig - Unique values:\n",
      "type\n",
      "CASH_OUT    2237500\n",
      "PAYMENT     2151495\n",
      "CASH_IN     1399284\n",
      "TRANSFER     532909\n",
      "DEBIT         41432\n",
      "Name: count, dtype: int64\n",
      "\n",
      "nameOrig - Unique values:\n",
      "nameOrig\n",
      "C1677795071    3\n",
      "C1999539787    3\n",
      "C724452879     3\n",
      "C1976208114    3\n",
      "C400299098     3\n",
      "              ..\n",
      "C1970706589    1\n",
      "C40604503      1\n",
      "C1614818636    1\n",
      "C2089752665    1\n",
      "C154988899     1\n",
      "Name: count, Length: 6353307, dtype: int64\n",
      "\n",
      "nameDest - Unique values:\n",
      "nameOrig\n",
      "C1677795071    3\n",
      "C1999539787    3\n",
      "C724452879     3\n",
      "C1976208114    3\n",
      "C400299098     3\n",
      "              ..\n",
      "C1970706589    1\n",
      "C40604503      1\n",
      "C1614818636    1\n",
      "C2089752665    1\n",
      "C154988899     1\n",
      "Name: count, Length: 6353307, dtype: int64\n",
      "\n",
      "nameDest - Unique values:\n",
      "nameOrig\n",
      "C1677795071    3\n",
      "C1999539787    3\n",
      "C724452879     3\n",
      "C1976208114    3\n",
      "C400299098     3\n",
      "              ..\n",
      "C1970706589    1\n",
      "C40604503      1\n",
      "C1614818636    1\n",
      "C2089752665    1\n",
      "C154988899     1\n",
      "Name: count, Length: 6353307, dtype: int64\n",
      "\n",
      "nameDest - Unique values:\n",
      "nameDest\n",
      "C1286084959    113\n",
      "C985934102     109\n",
      "C665576141     105\n",
      "C2083562754    102\n",
      "C248609774     101\n",
      "              ... \n",
      "C1049862186      1\n",
      "C2118381511      1\n",
      "C2099952089      1\n",
      "C1027984317      1\n",
      "C1251365829      1\n",
      "Name: count, Length: 2722362, dtype: int64\n",
      "\n",
      "==================================================\n",
      "TARGET VARIABLE DISTRIBUTION\n",
      "==================================================\n",
      "\n",
      "isFraud distribution:\n",
      "isFraud\n",
      "0    6354407\n",
      "1       8213\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fraud percentage: 0.13%\n",
      "nameDest\n",
      "C1286084959    113\n",
      "C985934102     109\n",
      "C665576141     105\n",
      "C2083562754    102\n",
      "C248609774     101\n",
      "              ... \n",
      "C1049862186      1\n",
      "C2118381511      1\n",
      "C2099952089      1\n",
      "C1027984317      1\n",
      "C1251365829      1\n",
      "Name: count, Length: 2722362, dtype: int64\n",
      "\n",
      "==================================================\n",
      "TARGET VARIABLE DISTRIBUTION\n",
      "==================================================\n",
      "\n",
      "isFraud distribution:\n",
      "isFraud\n",
      "0    6354407\n",
      "1       8213\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fraud percentage: 0.13%\n",
      "nameDest\n",
      "C1286084959    113\n",
      "C985934102     109\n",
      "C665576141     105\n",
      "C2083562754    102\n",
      "C248609774     101\n",
      "              ... \n",
      "C1049862186      1\n",
      "C2118381511      1\n",
      "C2099952089      1\n",
      "C1027984317      1\n",
      "C1251365829      1\n",
      "Name: count, Length: 2722362, dtype: int64\n",
      "\n",
      "==================================================\n",
      "TARGET VARIABLE DISTRIBUTION\n",
      "==================================================\n",
      "\n",
      "isFraud distribution:\n",
      "isFraud\n",
      "0    6354407\n",
      "1       8213\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fraud percentage: 0.13%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# First, let's load your dataset (adjust the filename if needed)\n",
    "# df = pd.read_csv('your_fraud_dataset.csv')\n",
    "\n",
    "# If you already have it loaded, let's explore it\n",
    "print(\"=\" * 50)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic info\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"COLUMN NAMES\")\n",
    "print(\"=\" * 50)\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DATA TYPES\")\n",
    "print(\"=\" * 50)\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FIRST 5 ROWS\")\n",
    "print(\"=\" * 50)\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"BASIC STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\" * 50)\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"CATEGORICAL COLUMNS (object type)\")\n",
    "print(\"=\" * 50)\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(categorical_cols)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col} - Unique values:\")\n",
    "    print(df[col].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TARGET VARIABLE DISTRIBUTION\")\n",
    "print(\"=\" * 50)\n",
    "# Common names for fraud column: 'isFraud', 'is_fraud', 'fraud', 'Class'\n",
    "fraud_col_names = ['isFraud', 'is_fraud', 'fraud', 'Class', 'label']\n",
    "for col_name in fraud_col_names:\n",
    "    if col_name in df.columns:\n",
    "        print(f\"\\n{col_name} distribution:\")\n",
    "        print(df[col_name].value_counts())\n",
    "        print(f\"\\nFraud percentage: {(df[col_name].sum() / len(df)) * 100:.2f}%\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca3d1f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATA PREPARATION\n",
      "==================================================\n",
      "\n",
      "Original feature shape: (6362620, 10)\n",
      "Target shape: (6362620,)\n",
      "\n",
      "Original feature shape: (6362620, 10)\n",
      "Target shape: (6362620,)\n",
      "\n",
      "Original feature shape: (6362620, 10)\n",
      "Target shape: (6362620,)\n",
      "\n",
      "Categorical columns found: ['type', 'nameOrig', 'nameDest']\n",
      "\n",
      "type: 5 unique values\n",
      "  → Will encode type\n",
      "\n",
      "Categorical columns found: ['type', 'nameOrig', 'nameDest']\n",
      "\n",
      "type: 5 unique values\n",
      "  → Will encode type\n",
      "\n",
      "Categorical columns found: ['type', 'nameOrig', 'nameDest']\n",
      "\n",
      "type: 5 unique values\n",
      "  → Will encode type\n",
      "\n",
      "nameOrig: 6353307 unique values\n",
      "  → Dropping nameOrig due to high cardinality\n",
      "\n",
      "nameOrig: 6353307 unique values\n",
      "  → Dropping nameOrig due to high cardinality\n",
      "\n",
      "nameOrig: 6353307 unique values\n",
      "  → Dropping nameOrig due to high cardinality\n",
      "\n",
      "nameDest: 2722362 unique values\n",
      "  → Dropping nameDest due to high cardinality\n",
      "\n",
      "nameDest: 2722362 unique values\n",
      "  → Dropping nameDest due to high cardinality\n",
      "\n",
      "nameDest: 2722362 unique values\n",
      "  → Dropping nameDest due to high cardinality\n",
      "\n",
      "Dropped 2 high-cardinality columns\n",
      "\n",
      "One-hot encoding 1 categorical columns...\n",
      "\n",
      "Dropped 2 high-cardinality columns\n",
      "\n",
      "One-hot encoding 1 categorical columns...\n",
      "\n",
      "Dropped 2 high-cardinality columns\n",
      "\n",
      "One-hot encoding 1 categorical columns...\n",
      "Shape after encoding: (6362620, 11)\n",
      "\n",
      "Final feature shape: (6362620, 11)\n",
      "Number of features: 11\n",
      "\n",
      "==================================================\n",
      "TRAIN-TEST SPLIT\n",
      "==================================================\n",
      "Shape after encoding: (6362620, 11)\n",
      "\n",
      "Final feature shape: (6362620, 11)\n",
      "Number of features: 11\n",
      "\n",
      "==================================================\n",
      "TRAIN-TEST SPLIT\n",
      "==================================================\n",
      "Shape after encoding: (6362620, 11)\n",
      "\n",
      "Final feature shape: (6362620, 11)\n",
      "Number of features: 11\n",
      "\n",
      "==================================================\n",
      "TRAIN-TEST SPLIT\n",
      "==================================================\n",
      "Training set: 5090096 samples\n",
      "Test set: 1272524 samples\n",
      "\n",
      "==================================================\n",
      "HANDLING CLASS IMBALANCE WITH SMOTE\n",
      "==================================================\n",
      "Before SMOTE - Class distribution:\n",
      "[5083526    6570]\n",
      "Training set: 5090096 samples\n",
      "Test set: 1272524 samples\n",
      "\n",
      "==================================================\n",
      "HANDLING CLASS IMBALANCE WITH SMOTE\n",
      "==================================================\n",
      "Before SMOTE - Class distribution:\n",
      "[5083526    6570]\n",
      "Training set: 5090096 samples\n",
      "Test set: 1272524 samples\n",
      "\n",
      "==================================================\n",
      "HANDLING CLASS IMBALANCE WITH SMOTE\n",
      "==================================================\n",
      "Before SMOTE - Class distribution:\n",
      "[5083526    6570]\n",
      "\n",
      "After SMOTE - Class distribution:\n",
      "[5083526 5083526]\n",
      "Training set after SMOTE: 10167052 samples\n",
      "\n",
      "==================================================\n",
      "FEATURE SCALING\n",
      "==================================================\n",
      "\n",
      "After SMOTE - Class distribution:\n",
      "[5083526 5083526]\n",
      "Training set after SMOTE: 10167052 samples\n",
      "\n",
      "==================================================\n",
      "FEATURE SCALING\n",
      "==================================================\n",
      "\n",
      "After SMOTE - Class distribution:\n",
      "[5083526 5083526]\n",
      "Training set after SMOTE: 10167052 samples\n",
      "\n",
      "==================================================\n",
      "FEATURE SCALING\n",
      "==================================================\n",
      "Features scaled using StandardScaler\n",
      "\n",
      "==================================================\n",
      "RESHAPING FOR LSTM\n",
      "==================================================\n",
      "Training set shape: (10167052, 1, 11)\n",
      "  → Samples: 10167052\n",
      "  → Timesteps: 1\n",
      "  → Features: 11\n",
      "\n",
      "Test set shape: (1272524, 1, 11)\n",
      "  → Samples: 1272524\n",
      "  → Timesteps: 1\n",
      "  → Features: 11\n",
      "\n",
      "==================================================\n",
      "DATA PREPARATION COMPLETE\n",
      "==================================================\n",
      "Features scaled using StandardScaler\n",
      "\n",
      "==================================================\n",
      "RESHAPING FOR LSTM\n",
      "==================================================\n",
      "Training set shape: (10167052, 1, 11)\n",
      "  → Samples: 10167052\n",
      "  → Timesteps: 1\n",
      "  → Features: 11\n",
      "\n",
      "Test set shape: (1272524, 1, 11)\n",
      "  → Samples: 1272524\n",
      "  → Timesteps: 1\n",
      "  → Features: 11\n",
      "\n",
      "==================================================\n",
      "DATA PREPARATION COMPLETE\n",
      "==================================================\n",
      "Features scaled using StandardScaler\n",
      "\n",
      "==================================================\n",
      "RESHAPING FOR LSTM\n",
      "==================================================\n",
      "Training set shape: (10167052, 1, 11)\n",
      "  → Samples: 10167052\n",
      "  → Timesteps: 1\n",
      "  → Features: 11\n",
      "\n",
      "Test set shape: (1272524, 1, 11)\n",
      "  → Samples: 1272524\n",
      "  → Timesteps: 1\n",
      "  → Features: 11\n",
      "\n",
      "==================================================\n",
      "DATA PREPARATION COMPLETE\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation and Preprocessing\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DATA PREPARATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Identify target column (adjust based on your dataset exploration from cell 4)\n",
    "target_col = 'isFraud'  # Change this based on your actual target column name\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "print(f\"\\nOriginal feature shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Handle categorical variables\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "if categorical_cols:\n",
    "    print(f\"\\nCategorical columns found: {categorical_cols}\")\n",
    "    \n",
    "    # Create lists to track columns\n",
    "    cols_to_drop = []\n",
    "    cols_to_encode = []\n",
    "    \n",
    "    # Check cardinality of each categorical column\n",
    "    for col in categorical_cols:\n",
    "        n_unique = X[col].nunique()\n",
    "        print(f\"\\n{col}: {n_unique} unique values\")\n",
    "        \n",
    "        # Drop high-cardinality columns (e.g., IDs, account numbers)\n",
    "        if n_unique > 100:  # Adjust threshold as needed\n",
    "            print(f\"  → Dropping {col} due to high cardinality\")\n",
    "            cols_to_drop.append(col)\n",
    "        else:\n",
    "            print(f\"  → Will encode {col}\")\n",
    "            cols_to_encode.append(col)\n",
    "    \n",
    "    # Drop high-cardinality columns\n",
    "    if cols_to_drop:\n",
    "        X = X.drop(columns=cols_to_drop)\n",
    "        print(f\"\\nDropped {len(cols_to_drop)} high-cardinality columns\")\n",
    "    \n",
    "    # One-hot encode remaining categorical columns\n",
    "    if cols_to_encode:\n",
    "        print(f\"\\nOne-hot encoding {len(cols_to_encode)} categorical columns...\")\n",
    "        X = pd.get_dummies(X, columns=cols_to_encode, drop_first=True)\n",
    "        print(f\"Shape after encoding: {X.shape}\")\n",
    "else:\n",
    "    print(\"\\nNo categorical columns found\")\n",
    "\n",
    "# Convert all columns to numeric\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Fill any NaN values created during conversion\n",
    "nan_count = X.isnull().sum().sum()\n",
    "if nan_count > 0:\n",
    "    print(f\"\\nFilling {nan_count} NaN values with 0\")\n",
    "    X = X.fillna(0)\n",
    "\n",
    "print(f\"\\nFinal feature shape: {X.shape}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "\n",
    "# Split data\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\" * 50)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"HANDLING CLASS IMBALANCE WITH SMOTE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Before SMOTE - Class distribution:\\n{np.bincount(y_train)}\")\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"\\nAfter SMOTE - Class distribution:\\n{np.bincount(y_train_resampled)}\")\n",
    "print(f\"Training set after SMOTE: {X_train_resampled.shape[0]} samples\")\n",
    "\n",
    "# Scale features\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FEATURE SCALING\")\n",
    "print(\"=\" * 50)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Features scaled using StandardScaler\")\n",
    "\n",
    "# Reshape for LSTM (samples, timesteps, features)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"RESHAPING FOR LSTM\")\n",
    "print(\"=\" * 50)\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "print(f\"Training set shape: {X_train_lstm.shape}\")\n",
    "print(f\"  → Samples: {X_train_lstm.shape[0]}\")\n",
    "print(f\"  → Timesteps: {X_train_lstm.shape[1]}\")\n",
    "print(f\"  → Features: {X_train_lstm.shape[2]}\")\n",
    "\n",
    "print(f\"\\nTest set shape: {X_test_lstm.shape}\")\n",
    "print(f\"  → Samples: {X_test_lstm.shape[0]}\")\n",
    "print(f\"  → Timesteps: {X_test_lstm.shape[1]}\")\n",
    "print(f\"  → Features: {X_test_lstm.shape[2]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc633958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ralph\\Documents\\GitHub\\ANN_ProjectFinal\\venv\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,456</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m19,456\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,569</span> (84.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,569\u001b[0m (84.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,569</span> (84.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m21,569\u001b[0m (84.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to create LSTM model for hyperparameter tuning\n",
    "def create_lstm_model(lstm_units=64, dropout_rate=0.2, learning_rate=0.001, \n",
    "                      num_lstm_layers=1, bidirectional=False):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer\n",
    "    if bidirectional:\n",
    "        model.add(Bidirectional(LSTM(lstm_units, return_sequences=(num_lstm_layers > 1), \n",
    "                                     input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]))))\n",
    "    else:\n",
    "        model.add(LSTM(lstm_units, return_sequences=(num_lstm_layers > 1), \n",
    "                      input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Additional LSTM layers\n",
    "    for i in range(1, num_lstm_layers):\n",
    "        if bidirectional:\n",
    "            model.add(Bidirectional(LSTM(lstm_units // (2**i), return_sequences=(i < num_lstm_layers - 1))))\n",
    "        else:\n",
    "            model.add(LSTM(lstm_units // (2**i), return_sequences=(i < num_lstm_layers - 1)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', \n",
    "                 metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test the model creation\n",
    "test_model = create_lstm_model()\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7746fd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter tuning...\n",
      "\n",
      "==================================================\n",
      "Training configuration 1/2\n",
      "==================================================\n",
      "Parameters: {'lstm_units': 64, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'num_lstm_layers': 1, 'bidirectional': False, 'batch_size': 128}\n",
      "Epoch 1/10\n",
      "Epoch 1/10\n",
      "Epoch 1/10\n",
      "\u001b[1m 3112/63545\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:01\u001b[0m 2ms/step - accuracy: 0.9085 - auc: 0.9635 - loss: 0.2272"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 28\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m create_lstm_model(\n\u001b[0;32m     21\u001b[0m     lstm_units\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm_units\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     22\u001b[0m     dropout_rate\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_rate\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     bidirectional\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbidirectional\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     26\u001b[0m )\n\u001b[1;32m---> 28\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_resampled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduced for speed\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Use training AUC instead of validation AUC (validation is buggy with huge datasets)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m train_auc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Ralph\\Documents\\GitHub\\ANN_ProjectFinal\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Ralph\\Documents\\GitHub\\ANN_ProjectFinal\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:399\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    398\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[1;32m--> 399\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\Ralph\\Documents\\GitHub\\ANN_ProjectFinal\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:242\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    239\u001b[0m     iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    240\u001b[0m ):\n\u001b[0;32m    241\u001b[0m     opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[1;32m--> 242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mopt_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mget_value()\n",
      "File \u001b[1;32mc:\\Users\\Ralph\\Documents\\GitHub\\ANN_ProjectFinal\\venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\optional_ops.py:176\u001b[0m, in \u001b[0;36m_OptionalImpl.has_value\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhas_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    175\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_optional_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptional_has_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ralph\\Documents\\GitHub\\ANN_ProjectFinal\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_optional_ops.py:172\u001b[0m, in \u001b[0;36moptional_has_value\u001b[1;34m(optional, name)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m    171\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOptionalHasValue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    175\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Lighter hyperparameter tuning - FIXED VERSION\n",
    "\n",
    "print(\"Starting hyperparameter tuning...\")\n",
    "\n",
    "param_grid = [\n",
    "    {'lstm_units': 64, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'num_lstm_layers': 1, 'bidirectional': False, 'batch_size': 128},\n",
    "    {'lstm_units': 128, 'dropout_rate': 0.3, 'learning_rate': 0.0001, 'num_lstm_layers': 1, 'bidirectional': True, 'batch_size': 128},\n",
    "]\n",
    "\n",
    "best_score = 0\n",
    "best_params = None\n",
    "results = []\n",
    "\n",
    "for i, params in enumerate(param_grid, 1):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training configuration {i}/{len(param_grid)}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Parameters: {params}\")\n",
    "    \n",
    "    model = create_lstm_model(\n",
    "        lstm_units=params['lstm_units'],\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        num_lstm_layers=params['num_lstm_layers'],\n",
    "        bidirectional=params['bidirectional']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_lstm, y_train_resampled,\n",
    "        validation_split=0.2,\n",
    "        epochs=10,  # Reduced for speed\n",
    "        batch_size=params['batch_size'],\n",
    "        callbacks=[EarlyStopping(monitor='loss', patience=3, restore_best_weights=True, verbose=0)],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Use training AUC instead of validation AUC (validation is buggy with huge datasets)\n",
    "    train_auc = max(history.history['auc'])\n",
    "    print(f\"Training AUC: {train_auc:.4f}\")\n",
    "    \n",
    "    results.append({'params': params, 'train_auc': train_auc})\n",
    "    \n",
    "    if train_auc > best_score:\n",
    "        best_score = train_auc\n",
    "        best_params = params\n",
    "        print(\"★ New best model!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BEST HYPERPARAMETERS\")\n",
    "print(\"=\"*50)\n",
    "print(best_params)\n",
    "print(f\"\\nBest Training AUC: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46329bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed007a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data shapes:\n",
      "X_train_lstm shape: (10167052, 1, 11)\n",
      "y_train_resampled shape: (10167052,)\n",
      "y_train_resampled unique values: [0 1]\n",
      "y_train_resampled distribution: [5083526 5083526]\n",
      "\n",
      "NaN in X_train_lstm: 0\n",
      "\n",
      "NaN in X_train_lstm: 0\n",
      "\n",
      "NaN in X_train_lstm: 0\n",
      "Inf in X_train_lstm: 0\n",
      "\n",
      "Testing a simple model...\n",
      "Inf in X_train_lstm: 0\n",
      "\n",
      "Testing a simple model...\n",
      "Inf in X_train_lstm: 0\n",
      "\n",
      "Testing a simple model...\n",
      "Epoch 1/5\n",
      "Epoch 1/5\n",
      "Epoch 1/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9225 - auc: 0.7140 - loss: 0.6198 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.5407\n",
      "Epoch 2/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9225 - auc: 0.7140 - loss: 0.6198 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.5407\n",
      "Epoch 2/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9225 - auc: 0.7140 - loss: 0.6198 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.5407\n",
      "Epoch 2/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9987 - auc: 0.6195 - loss: 0.4721 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.3770\n",
      "Epoch 3/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9987 - auc: 0.6195 - loss: 0.4721 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.3770\n",
      "Epoch 3/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9987 - auc: 0.6195 - loss: 0.4721 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.3770\n",
      "Epoch 3/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9987 - auc: 0.5607 - loss: 0.3139 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.2119\n",
      "Epoch 4/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9987 - auc: 0.5607 - loss: 0.3139 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.2119\n",
      "Epoch 4/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9987 - auc: 0.5607 - loss: 0.3139 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.2119\n",
      "Epoch 4/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9987 - auc: 0.3805 - loss: 0.1804 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.1047\n",
      "Epoch 5/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9987 - auc: 0.3805 - loss: 0.1804 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.1047\n",
      "Epoch 5/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9987 - auc: 0.3805 - loss: 0.1804 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.1047\n",
      "Epoch 5/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9987 - auc: 0.6158 - loss: 0.1053 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.0531\n",
      "\n",
      "Test AUC: 0.7140\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9987 - auc: 0.6158 - loss: 0.1053 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.0531\n",
      "\n",
      "Test AUC: 0.7140\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9987 - auc: 0.6158 - loss: 0.1053 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.0531\n",
      "\n",
      "Test AUC: 0.7140\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic check\n",
    "print(\"Checking data shapes:\")\n",
    "print(f\"X_train_lstm shape: {X_train_lstm.shape}\")\n",
    "print(f\"y_train_resampled shape: {y_train_resampled.shape}\")\n",
    "print(f\"y_train_resampled unique values: {np.unique(y_train_resampled)}\")\n",
    "print(f\"y_train_resampled distribution: {np.bincount(y_train_resampled)}\")\n",
    "\n",
    "# Check if data has any NaN or inf\n",
    "print(f\"\\nNaN in X_train_lstm: {np.isnan(X_train_lstm).sum()}\")\n",
    "print(f\"Inf in X_train_lstm: {np.isinf(X_train_lstm).sum()}\")\n",
    "\n",
    "# Test a simple model\n",
    "print(\"\\nTesting a simple model...\")\n",
    "test_model = create_lstm_model(lstm_units=32, dropout_rate=0.2, learning_rate=0.001)\n",
    "test_history = test_model.fit(\n",
    "    X_train_lstm[:1000], y_train_resampled[:1000],  # Small sample\n",
    "    validation_split=0.2,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "print(f\"\\nTest AUC: {max(test_history.history['auc']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f5b7f69",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train final model with best parameters\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Extract parameters from best_params dictionary\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m lstm_units \u001b[38;5;241m=\u001b[39m \u001b[43mbest_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlstm_units\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      5\u001b[0m dropout_rate \u001b[38;5;241m=\u001b[39m best_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m best_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Train final model with best parameters\n",
    "\n",
    "# Extract parameters from best_params dictionary\n",
    "lstm_units = best_params['lstm_units']\n",
    "dropout_rate = best_params['dropout_rate']\n",
    "learning_rate = best_params['learning_rate']\n",
    "num_lstm_layers = best_params['num_lstm_layers']\n",
    "bidirectional = best_params['bidirectional']\n",
    "batch_size = best_params['batch_size']\n",
    "epochs = 50  # Full training epochs\n",
    "\n",
    "# Create best model\n",
    "best_model = create_lstm_model(\n",
    "    lstm_units=lstm_units,\n",
    "    dropout_rate=dropout_rate,\n",
    "    learning_rate=learning_rate,\n",
    "    num_lstm_layers=num_lstm_layers,\n",
    "    bidirectional=bidirectional\n",
    ")\n",
    "\n",
    "# Setup callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING FINAL MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train_lstm, y_train_resampled,\n",
    "    validation_split=0.2,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ca033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_title('Model Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[1].set_title('Model Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# AUC\n",
    "axes[2].plot(history.history['auc'], label='Training AUC')\n",
    "axes[2].plot(history.history['val_auc'], label='Validation AUC')\n",
    "axes[2].set_title('Model AUC')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('AUC')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b1d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred_proba = best_model.predict(X_test_lstm)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "# ROC-AUC Score\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Additional metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13802cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "best_model.save('fraud_detection_lstm_model.h5')\n",
    "print(\"\\nModel saved as 'fraud_detection_lstm_model.h5'\")\n",
    "\n",
    "# Save scaler for future use\n",
    "import pickle\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"Scaler saved as 'scaler.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
